---
title: "dataDownload"
format: html
editor: visual
---

## basic table mapping

```{r}
dataset_table_mapping <- list(
  BIDMOVE_COMPLETE = c("BIDDAYOFFER_D", "BIDPEROFFER_D"),
  OPERATIONAL_DEMAND = "OPERATIONAL_DEMAND",
  ROOFTOP = "ROOFTOP",
  NEXT_DAY_DISPATCH = c("CONSTRAINT", "LOCAL_PRICE", "MNSPBIDTRK", "OFFERTRK", "UNIT_SOLUTION"),
  PUBLIC_PRICES = "PUBLIC_PRICES",
  NETWORK = c("EQUIPMENTDETAIL", "OUTAGECONSTRAINTSET", "OUTAGEDETAIL", "OUTAGESTATUSCODE", "RATING", "STATICRATING", "SUBSTATIONDETAIL"),
  NEXT_DAY_INTERMITTENT_DS = c("INTERMITTENT_DS_RUN", "INTERMITTENT_DS_PRED", "INTERMITTENT_FORECAST_TRK")
)

saveRDS(dataset_table_mapping, "~/Documents/internshipSEC/EnergyMarketAnalysis-R/dataset_table_mapping.rds")

```

## Downloader + Extractor Zip to CSV + Parquet Converter from CSV for AEMO Data

### WOrking final

```{r}
# Combined Efficient Downloader + Extractor + Parquet Converter for AEMO Data

library(rvest)
library(stringr)
library(httr)
library(fs)
library(purrr)
library(tools)
library(data.table)
library(arrow)
library(progressr)

library(future)
library(future.apply)

options(arrow.use_threads = TRUE)

handlers(global = TRUE)
handlers("txtprogressbar")

base_archive      <- "https://nemweb.com.au/Reports/Archive/"
root_page         <- read_html(base_archive)

local_data_base   <- "data"
parquet_output    <- "parquet_tables"
download_timeout  <- 600

plan(multicore, workers = max(1, parallel::detectCores() - 1))

# Load dataset-to-table mapping
mapping_rds <- "dataset_table_mapping.rds"
if (file.exists(mapping_rds)) {
  table_map <- readRDS(mapping_rds)
} else {
  table_map <- list()
  saveRDS(table_map, mapping_rds)
}

# Function to update and save table map
update_table_map <- function(dataset_name, new_table_names) {
  dataset_upper <- toupper(dataset_name)
  existing      <- table_map[[dataset_upper]] %||% character()
  to_add        <- setdiff(new_table_names, existing)
  if (length(to_add) == 0) return(invisible(NULL))
  table_map[[dataset_upper]] <<- c(existing, to_add)
  saveRDS(table_map, mapping_rds)
  message("📌 Added to ", dataset_upper, ": ", paste(to_add, collapse = ", "))
}

# Step 1: Choose dataset
dataset_folders <- html_elements(root_page, "a") %>%
  html_attr("href") %>%
  str_subset("/$") %>%
  discard(~ .x %in% c("../")) %>%
  str_remove("^/Reports/Archive/")

cat("\U0001F4E6 Available dataset folders from AEMO Archive:\n")
for (i in seq_along(dataset_folders)) {
  cat(sprintf("[%02d] %s\n", i, dataset_folders[[i]]))
}
index        <- as.integer(readline("\U0001F522 Enter the number of the dataset folder to download from: "))
dataset_path <- dataset_folders[index]
current_url  <- paste0(base_archive, dataset_path)
dataset_name <- gsub("/$", "", dataset_path)
dataset_upper<- toupper(dataset_name)

local_base   <- file.path(local_data_base, dataset_name)
output_base  <- parquet_output

# Determine which tables to check for existing parquet files
expected_tables <- table_map[[dataset_upper]] %||% character()
expected_dirs   <- file.path(output_base, expected_tables)

# Create necessary directories
if (!dir_exists(local_base)) dir_create(local_base)
if (!dir_exists(output_base)) dir_create(output_base)

# Scan all expected parquet dirs for existing dates
existing_parquet_dates <- unique(unlist(
  lapply(expected_dirs[file.exists(expected_dirs)], function(dir) {
    dir(dir, pattern = "\\.parquet$", full.names = TRUE) %>%
      basename() %>%
      str_extract("\\d{8}")
  })
)) %>% na.omit() %>% unique()

# Helper: extract tables from a CSV-like file with multiple I, rows
extract_id_tables <- function(file_path) {
  raw_lines      <- readLines(file_path, warn = FALSE)
  header_indices <- grep("^I,", raw_lines, ignore.case = TRUE)
  if (length(header_indices) == 0) return(NULL)

  tables       <- list()
  total_tables <- length(header_indices)

  get_dataset_name <- function(path) {
    parts      <- str_split(normalizePath(path), .Platform$file.sep)[[1]]
    data_index <- which(parts == "data")
    if (length(data_index) > 0 && length(parts) > data_index)
      toupper(parts[data_index + 1])
    else
      "UNKNOWNTABLE"
  }
  fallback_name <- get_dataset_name(file_path)

  for (i in seq_along(header_indices)) {
    start_idx  <- header_indices[i]
    end_idx    <- if (i < length(header_indices)) header_indices[i + 1] - 1 else length(raw_lines)
    table_lines<- raw_lines[start_idx:end_idx]
    table_data <- fread(text = paste(table_lines, collapse = "\n"), fill = TRUE, showProgress = FALSE)
    header_row <- fread(text = raw_lines[start_idx], fill = TRUE, header = FALSE)
    name_col   <- if (total_tables == 1) 2 else 3
    table_name <- gsub("[^A-Za-z0-9_]", "_", as.character(header_row[[name_col]]))
    table_name <- toupper(table_name)
    if (is.na(table_name) || table_name == "") table_name <- fallback_name

    if (!is.null(tables[[table_name]])) {
      tables[[table_name]] <- rbind(tables[[table_name]], table_data, fill = TRUE)
    } else {
      tables[[table_name]] <- table_data
    }
  }
  tables
}

# Walk down into the archive until we find ZIPs
navigate_until_zip <- function(url, level = 1) {
  page        <- read_html(url)
  links       <- html_elements(page, "a") %>% html_attr("href") %>% discard(is.na)
  zip_links   <- links %>% str_subset("\\.zip$")
  folder_links<- links %>% str_subset("/$") %>% discard(~ .x %in% c("../"))

  if (length(zip_links) > 0) return(list(zip_files = zip_links, final_url = url))
  if (length(folder_links) == 0) stop("❌ No zip files or subfolders found.")
  cat("\n", strrep("—", level), "\U0001F4C1 Subfolders:\n", sep = "")
  folder_names <- basename(str_remove(folder_links, "/$"))
  for (i in seq_along(folder_links)) cat(sprintf("[%02d] %s\n", i, folder_names[i]))
  sub_index   <- as.integer(readline("\U0001F522 Select a subfolder: "))
  next_url    <- url_absolute(folder_links[sub_index], url)
  navigate_until_zip(next_url, level + 1)
}

# Find our ZIP listings and dates
result         <- navigate_until_zip(current_url)
zip_files      <- result$zip_files
final_url      <- result$final_url
available_dates<- str_extract(zip_files, "\\d{8}") %>% na.omit() %>% unique() %>% sort()

# Also track any CSV-only dates (in case of reprocessing)
existing_csv_dirs  <- dir(local_base, full.names = TRUE, recursive = FALSE)
existing_csv_dates <- existing_csv_dirs %>%
  keep(~ length(fs::dir_ls(.x, recurse = TRUE, regexp = "(?i)\\.csv$", type = "file")) > 0) %>%
  basename() %>%
  str_extract("\\d{8}") %>%
  na.omit() %>%
  unique()

# What still needs doing?
processable_dates <- setdiff(available_dates, union(existing_parquet_dates, existing_csv_dates))
reprocess_dates  <- setdiff(existing_csv_dates, existing_parquet_dates)

if (length(processable_dates) == 0 && length(reprocess_dates) == 0) {
  message("✅ You are already up to date. Nothing to download.")
  return(invisible(NULL))
}
if (length(processable_dates) > 0) {
  cat("\n📅 Dates to be downloaded and processed:\n"); print(processable_dates)
}
if (length(reprocess_dates) > 0) {
  cat("\n🔄 Dates with CSVs but missing parquet (to be reprocessed):\n"); print(reprocess_dates)
}

all_dates   <- c(processable_dates, reprocess_dates)
user_input  <- readline("📥 Press Enter to process all, or enter a specific YYYYMMDD date: ")
missing_dates <- if (user_input == "") all_dates else if (user_input %in% all_dates) user_input else stop("❌ That date is not available.")

with_progress({
  p <- progressor(along = missing_dates)

  for (date in missing_dates) {
    tryCatch({
      # —————— Parquet‐exists guard ——————
      if (date %in% existing_parquet_dates) {
        message("⏭️  Parquet for ", date, " already exists; skipping.")
        p()
        next
      }

      zip_match  <- zip_files[str_detect(zip_files, date)]
      if (length(zip_match) == 0) {
        message("⚠️ No zip found for date: ", date)
        p()
        next
      }

      file_name   <- basename(zip_match[1])
      full_url    <- paste0(final_url, file_name)
      zip_path    <- file.path(tempdir(), file_name)
      zip_base    <- file_path_sans_ext(file_name)
      extract_dir <- file.path(local_base, zip_base)

      new_download <- FALSE
      # Download & unzip if needed
      if (!dir_exists(extract_dir) || length(dir(extract_dir, "\\.csv$", recursive = TRUE)) == 0) {
        message("⬇️  Downloading and extracting ", file_name)
        GET(full_url, write_disk(zip_path, overwrite = TRUE), timeout(download_timeout))
        dir_create(extract_dir)
        unzip(zip_path, exdir = extract_dir)
        inner_zips <- dir(extract_dir, pattern = "\\.zip$", full.names = TRUE)
        for (z in inner_zips) {
          unzip(z, exdir = extract_dir)
          file_delete(z)
        }
        new_download <- TRUE
      } else {
        message("📁 Using existing CSVs for ", date)
      }

      # Update mapping if new tables discovered
      if (new_download) {
        discovered_tables <- unique(unlist(lapply(
          dir(extract_dir, pattern="\\.csv$", full.names=TRUE, recursive=TRUE),
          function(file) {
            raw_lines      <- readLines(file, warn = FALSE)
            header_indices <- grep("^I,", raw_lines, ignore.case = TRUE)
            if (length(header_indices) == 0) return(NULL)
            total_tables   <- length(header_indices)
            sapply(seq_along(header_indices), function(i) {
              header_row <- fread(text = raw_lines[header_indices[i]], fill = TRUE, header = FALSE)
              name_col   <- if (total_tables == 1) 2 else 3
              tname      <- gsub("[^A-Za-z0-9_]", "_", as.character(header_row[[name_col]]))
              toupper(ifelse(is.na(tname)||tname=="","UNKNOWNTABLE", tname))
            })
          }
        )))
        update_table_map(dataset_name, discovered_tables)
      }

      # Extract and combine
      combined_tables <- list()
      csv_files       <- dir(extract_dir, pattern="\\.csv$", full.names=TRUE, recursive=TRUE)
      for (csv_file in csv_files) {
        tables <- extract_id_tables(csv_file)
        if (is.null(tables)) next
        for (nm in names(tables)) {
          if (!is.null(combined_tables[[nm]])) {
            combined_tables[[nm]] <- rbind(combined_tables[[nm]], tables[[nm]], fill=TRUE)
          } else {
            combined_tables[[nm]] <- tables[[nm]]
          }
        }
      }

      # Write out parquet per table
      for (nm in names(combined_tables)) {
        dir_create(file.path(output_base, nm))
        pq_path <- file.path(output_base, nm, paste0(zip_base, ".parquet"))
        write_parquet(combined_tables[[nm]], pq_path)
        message("✅ Saved: ", pq_path)
      }

      # Clean up CSVs
      file_delete(csv_files)
      message("🧹 Deleted original CSVs for ", date)
    }, error = function(e) {
      warning("❌ Error processing ", date, ": ", e$message)
    })

    p()
  }
})

message("🎉 All downloads, extraction, and parquet conversion completed.")

```
#not used
##not working

```{r}
library(rvest)
library(stringr)
library(httr)
library(fs)
library(purrr)
library(tools)
library(data.table)
library(arrow)
library(progressr)

handlers(global = TRUE)
handlers("txtprogressbar")

base_archive <- "https://nemweb.com.au/Reports/Archive/"
root_page <- read_html(base_archive)

# === Step 1: List top-level datasets ===
dataset_folders <- html_elements(root_page, "a") %>%
  html_attr("href") %>%
  str_subset("/$") %>%
  discard(~ .x %in% c("../")) %>%
  str_remove("^/Reports/Archive/")

cat("\U0001F4E6 Available dataset folders from AEMO Archive:\n")
for (i in seq_along(dataset_folders)) {
  cat(sprintf("[%02d] %s\n", i, dataset_folders[[i]]))
}
index <- as.integer(readline("\U0001F522 Enter the number of the dataset folder to download from: "))
dataset_path <- dataset_folders[index]
current_url <- paste0(base_archive, dataset_path)
dataset_name <- gsub("/$", "", dataset_path)

rtrim <- function(x, char = "/") sub(paste0(char, "$"), "", x)

extract_id_tables <- function(file_path) {
  raw_lines <- readLines(file_path, warn = FALSE)
  header_indices <- grep("^I,", raw_lines, ignore.case = TRUE)
  if (length(header_indices) == 0) return(NULL)

  tables <- list()
  total_tables <- length(header_indices)

  get_dataset_name <- function(path) {
    parts <- str_split(normalizePath(path), .Platform$file.sep)[[1]]
    data_index <- which(parts == "data")
    if (length(data_index) > 0 && length(parts) > data_index) toupper(parts[data_index + 1]) else "UNKNOWNTABLE"
  }

  fallback_name <- get_dataset_name(file_path)

  for (i in seq_along(header_indices)) {
    start_idx <- header_indices[i]
    end_idx <- if (i < length(header_indices)) header_indices[i + 1] - 1 else length(raw_lines)
    table_lines <- raw_lines[start_idx:end_idx]
    table_data <- fread(text = paste(table_lines, collapse = "\n"), fill = TRUE, showProgress = FALSE)
    header_row <- fread(text = raw_lines[start_idx], fill = TRUE, header = FALSE)
    name_col <- if (total_tables == 1) 2 else 3
    table_name <- gsub("[^A-Za-z0-9_]", "_", as.character(header_row[[name_col]]))
    table_name <- toupper(table_name)
    if (is.na(table_name) || table_name == "") table_name <- fallback_name
    tables[[table_name]] <- if (!is.null(tables[[table_name]])) rbind(tables[[table_name]], table_data, fill = TRUE) else table_data
  }
  tables
}

navigate_until_zip <- function(url, level = 1) {
  page <- read_html(url)
  links <- html_elements(page, "a") %>% html_attr("href") %>% discard(is.na)
  zip_links <- links %>% str_subset("\\.zip$")
  folder_links <- links %>% str_subset("/$") %>% discard(~ .x %in% c("../"))

  if (length(zip_links) > 0) return(list(zip_files = zip_links, final_url = url))
  if (length(folder_links) == 0) stop("\u274C No zip files or subfolders found.")
  cat("\n", strrep("—", level), "\U0001F4C1 Subfolders:\n", sep = "")
  folder_names <- basename(str_remove(folder_links, "/$"))
  for (i in seq_along(folder_links)) cat(sprintf("[%02d] %s\n", i, folder_names[i]))
  sub_index <- as.integer(readline("\U0001F522 Select a subfolder: "))
  next_url <- url_absolute(folder_links[sub_index], url)
  navigate_until_zip(next_url, level + 1)
}

result <- navigate_until_zip(current_url)
zip_files <- result$zip_files
final_url <- result$final_url

available_dates <- str_extract(zip_files, "\\d{8}") %>% na.omit() %>% unique() %>% sort()

# === Step: Determine missing/processable dates ===
local_base <- file.path("data", dataset_name)
output_base <- "parquet_tables"
dir_create(output_base)

dataset_upper <- toupper(dataset_name)
parquet_dataset_dir <- file.path(output_base, dataset_upper)

if (dir_exists(parquet_dataset_dir)) {
  existing_parquet_dates <- dir(parquet_dataset_dir, pattern = "\\.parquet$", full.names = TRUE) %>%
    basename() %>%
    str_extract("\\d{8}") %>%
    na.omit() %>%
    unique()
} else {
  existing_parquet_dates <- character(0)
}


existing_csv_dirs <- dir(local_base, full.names = TRUE, recursive = FALSE)

existing_csv_dates <- existing_csv_dirs %>%
  keep(~ {
    csvs <- fs::dir_ls(.x, recurse = TRUE, regexp = "(?i)\\.csv$", type = "file")
    length(csvs) > 0
  }) %>%
  basename() %>%
  str_extract("\\d{8}") %>%
  na.omit() %>%
  unique()


processable_dates <- setdiff(available_dates, union(existing_parquet_dates, existing_csv_dates))

# Check and reprocess dates with CSVs but missing parquet
reprocess_dates <- setdiff(existing_csv_dates, existing_parquet_dates)

if (length(processable_dates) == 0 && length(reprocess_dates) == 0) {
  message("\u2705 You are already up to date. Nothing to download.")
  return(invisible(NULL))
}

if (length(processable_dates) > 0) {
  cat("\n\U0001F4C5 Dates to be downloaded and processed:\n")
  print(processable_dates)
}
if (length(reprocess_dates) > 0) {
  cat("\n\U0001F504 Dates with CSVs but missing parquet (to be reprocessed):\n")
  print(reprocess_dates)
}

all_dates <- c(processable_dates, reprocess_dates)

# Only create local_base if necessary
if (!dir_exists(local_base)) dir_create(local_base)

user_input <- readline("\U0001F4E5 Press Enter to process all, or enter a specific YYYYMMDD date: ")
missing_dates <- if (user_input == "") all_dates else if (user_input %in% all_dates) user_input else stop("\u274C That date is not available.")

with_progress({
  p <- progressor(along = missing_dates)
  for (date in missing_dates) {
    zip_match <- zip_files[str_detect(zip_files, date)]
    if (length(zip_match) == 0) next

    file_name <- basename(zip_match[1])
    full_url <- paste0(final_url, file_name)
    zip_path <- file.path(tempdir(), file_name)
    zip_base <- file_path_sans_ext(file_name)
    extract_dir <- file.path(local_base, zip_base)

    if (!dir_exists(extract_dir) || length(dir(extract_dir, pattern = "(?i)\\.csv$", recursive = TRUE)) == 0) {
      tryCatch({
        if (!file_exists(zip_path)) {
          message("⬇️  Downloading: ", file_name)
          GET(full_url, write_disk(zip_path, overwrite = TRUE), timeout(600))
        }
        dir_create(extract_dir)
        unzip(zip_path, exdir = extract_dir)
        inner_zips <- dir(extract_dir, pattern = "\\.zip$", full.names = TRUE)
        for (z in inner_zips) {
          unzip(z, exdir = extract_dir)
          file_delete(z)
        }
      }, error = function(e) {
        warning("❌ Failed to extract: ", file_name, " — ", e$message)
        p()
        next
      })
    }

    csv_files <- dir(extract_dir, pattern = "(?i)\\.csv$", full.names = TRUE, recursive = TRUE)
    if (length(csv_files) == 0) {
      warning("⚠️ No CSVs found in: ", extract_dir)
      p()
      next
    }

    combined_tables <- list()
    for (csv_file in csv_files) {
      tables <- extract_id_tables(csv_file)
      if (is.null(tables)) next
      for (name in names(tables)) {
        if (!is.null(combined_tables[[name]])) {
          combined_tables[[name]] <- rbind(combined_tables[[name]], tables[[name]], fill = TRUE)
        } else {
          combined_tables[[name]] <- tables[[name]]
        }
      }
    }

    for (name in names(combined_tables)) {
      dir_create(file.path(output_base, name))
      parquet_path <- file.path(output_base, name, paste0(zip_base, ".parquet"))
      write_parquet(combined_tables[[name]], parquet_path)
      message("✅ Saved: ", parquet_path)
    }

    file_delete(csv_files)
    message("🧹 Deleted original CSVs for ", date)

    p()
  }
})

message("🎉 All downloads, extraction, and parquet conversion completed.")

```

## Seperate downloader, extractor and converter (not used) - working

```{r}
library(rvest)
library(stringr)
library(httr)
library(fs)
library(purrr)
library(tools)

base_archive <- "https://nemweb.com.au/Reports/Archive/"
root_page <- read_html(base_archive)

# === Step 1: List top-level datasets ===
dataset_folders <- html_elements(root_page, "a") %>%
  html_attr("href") %>%
  str_subset("/$") %>%
  discard(~ .x %in% c("../")) %>%
  str_remove("^/Reports/Archive/")

cat("📦 Available dataset folders from AEMO Archive:\n")
for (i in seq_along(dataset_folders)) {
  cat(sprintf("[%02d] %s\n", i, dataset_folders[[i]]))
}
index <- as.integer(readline("🔢 Enter the number of the dataset folder to download from: "))
dataset_path <- dataset_folders[index]
current_url <- paste0(base_archive, dataset_path)
dataset_name <- gsub("/$", "", dataset_path)

# === Helper: Remove trailing character (e.g., slash) ===
rtrim <- function(x, char = "/") {
  sub(paste0(char, "$"), "", x)
}

# === Recursively drill down to folder with ZIP files ===
navigate_until_zip <- function(url, level = 1) {
  page <- read_html(url)
  links <- html_elements(page, "a") %>% html_attr("href") %>% discard(is.na)

  zip_links <- links %>% str_subset("\\.zip$")
  folder_links <- links %>% str_subset("/$") %>% discard(~ .x %in% c("../"))

  if (length(zip_links) > 0) {
    return(list(zip_files = zip_links, final_url = url))
  }

  if (length(folder_links) == 0) stop("❌ No zip files or subfolders found.")

  cat("\n", strrep("—", level), "📁 Subfolders:\n", sep = "")
  folder_names <- basename(str_remove(folder_links, "/$"))
  for (i in seq_along(folder_links)) {
    cat(sprintf("[%02d] %s\n", i, folder_names[i]))
  }
  sub_index <- as.integer(readline("🔢 Select a subfolder: "))
  next_url <- url_absolute(folder_links[sub_index], url)
  navigate_until_zip(next_url, level + 1)
}

# === Step 2: Get ZIPs from nested structure ===
result <- navigate_until_zip(current_url)
zip_files <- result$zip_files
final_url <- result$final_url

# === Step 3: Extract available dates from ZIPs ===
available_dates <- str_extract(zip_files, "\\d{8}") %>% na.omit() %>% unique() %>% sort()
cat("\n📅 Available dates:\n")
print(missing_dates)

# === Step 4: Check existing files ===
local_base <- file.path("data", dataset_name)
existing_dirs <- dir(local_base, full.names = FALSE)
existing_dates <- str_extract(existing_dirs, "\\d{8}") %>% na.omit()

user_input <- readline("📥 Press Enter to download all missing dates, or enter a specific YYYYMMDD date: ")
if (user_input == "") {
  missing_dates <- setdiff(available_dates, existing_dates)
} else {
  if (!(user_input %in% available_dates)) stop("❌ That date is not available.")
  if (user_input %in% existing_dates) {
    message("✅ Date already downloaded: ", user_input)
    missing_dates <- character(0)
  } else {
    missing_dates <- user_input
  }
}

# === Step 5: Download + extract ===
for (date in missing_dates) {
  zip_match <- zip_files[str_detect(zip_files, date)]
  if (length(zip_match) == 0) next

  file_name <- basename(zip_match[1])
  full_url <- paste0(final_url, file_name)
  zip_path <- file.path(tempdir(), file_name)

  if (!file_exists(zip_path)) {
    message("⬇️  Downloading: ", file_name)
    tryCatch({
      download.file(full_url, zip_path, mode = "wb")
    }, error = function(e) {
      warning("❌ Failed to download ", file_name, ": ", e$message)
      next
    })
  }

  zip_base <- file_path_sans_ext(file_name)
  extract_dir <- file.path(local_base, zip_base)
  dir_create(extract_dir)
  unzip(zip_path, exdir = extract_dir)

  inner_zips <- dir(extract_dir, pattern = "\\.zip$", full.names = TRUE)
  for (z in inner_zips) {
    unzip(z, exdir = extract_dir)
    file_delete(z)
  }

  message("✅ Extracted CSVs for: ", date)
}

message("🎉 All missing data downloaded and extracted.")

```

```{r}
library(fs)
library(stringr)
library(data.table)
library(arrow)

# === Function: Extract all I/D tables from a CSV ===
extract_id_tables <- function(file_path) {
  raw_lines <- readLines(file_path, warn = FALSE)
  header_indices <- grep("^I,", raw_lines, ignore.case = TRUE)
  if (length(header_indices) == 0) return(NULL)

  tables <- list()
  total_tables <- length(header_indices)

  # Get fallback name from file path if needed
  get_dataset_name <- function(path) {
    parts <- str_split(normalizePath(path), .Platform$file.sep)[[1]]
    data_index <- which(parts == "data")
    if (length(data_index) > 0 && length(parts) > data_index) {
      return(toupper(parts[data_index + 1]))  # Use uppercase fallback
    } else {
      return("UNKNOWNTABLE")
    }
  }

  fallback_name <- get_dataset_name(file_path)

  for (i in seq_along(header_indices)) {
    start_idx <- header_indices[i]
    end_idx <- if (i < length(header_indices)) header_indices[i + 1] - 1 else length(raw_lines)

    table_lines <- raw_lines[start_idx:end_idx]
    table_data <- fread(text = paste(table_lines, collapse = "\n"), fill = TRUE, showProgress = FALSE)

    header_row <- fread(text = raw_lines[start_idx], fill = TRUE, header = FALSE)

    # Choose name from column 2 (if one table) or 3 (if multiple tables)
    name_col <- if (total_tables == 1) 2 else 3
    table_name <- gsub("[^A-Za-z0-9_]", "_", as.character(header_row[[name_col]]))
    table_name <- toupper(table_name)

    # Fallback if empty
    if (is.na(table_name) || table_name == "") {
      table_name <- fallback_name
    }

    if (!is.null(tables[[table_name]])) {
      tables[[table_name]] <- rbind(tables[[table_name]], table_data, fill = TRUE)
    } else {
      tables[[table_name]] <- table_data
    }
  }

  return(tables)
}




# === Step 1: Dataset folder selection ===
dataset_folders <- dir("data", full.names = TRUE)
dataset_folders <- dataset_folders[file.info(dataset_folders)$isdir]

cat("📁 Dataset folders:\n")
for (i in seq_along(dataset_folders)) {
  cat(sprintf("[%02d] %s\n", i, basename(dataset_folders[i])))
}
folder_index <- as.integer(readline("🔢 Choose a dataset category: "))
chosen_category <- dataset_folders[folder_index]

# === Step 2: Loop over subfolders (date folders) ===
subfolders <- dir(chosen_category, full.names = TRUE)
subfolders <- subfolders[file.info(subfolders)$isdir]

output_base <- "parquet_tables"
dir_create(output_base)

for (folder in subfolders) {
  folder_name <- basename(folder)
  csv_files <- dir(folder, pattern = "\\.csv$", full.names = TRUE, recursive = TRUE, ignore.case = TRUE)
  if (length(csv_files) == 0) next

  # Check if Parquet already exists for this folder (just use the first .CSV to infer table)
  sample_tables <- extract_id_tables(csv_files[1])
  if (is.null(sample_tables)) next

  # ✅ Updated check for existing files
  parquet_paths <- file.path(output_base, names(sample_tables), paste0(folder_name, ".parquet"))
  already_done <- all(file_exists(parquet_paths))

  if (already_done) {
    cat("⏩ Skipping folder (already processed):", folder_name, "\n")
    next
  }

  cat("🔄 Processing folder:", folder_name, "\n")
  combined_tables <- list()

  for (csv_file in csv_files) {
    tables <- extract_id_tables(csv_file)
    if (is.null(tables)) next

    for (name in names(tables)) {
      if (!is.null(combined_tables[[name]])) {
        combined_tables[[name]] <- rbind(combined_tables[[name]], tables[[name]], fill = TRUE)
      } else {
        combined_tables[[name]] <- tables[[name]]
      }
    }
  }

  for (name in names(combined_tables)) {
    dir_create(file.path(output_base, name))
    parquet_path <- file.path(output_base, name, paste0(folder_name, ".parquet"))

    write_parquet(combined_tables[[name]], parquet_path)
    cat("✅ Saved:", parquet_path, "\n")
  }
}


cat("🎉 Done. All folders processed.\n")

```

## old code not used

```{r}

library(arrow)
library(dplyr)

# Read the Parquet file
df <- read_parquet("parquet_tables/UnknownTable/PUBLIC_PRICES_20241201.parquet")

# Check number of unique SETTLEMENTDATEs
df %>%
  mutate(date = as.Date(SETTLEMENTDATE)) %>%
  summarise(
    num_dates = n_distinct(date),
    min_date = min(date),
    max_date = max(date)
  )

```

```{r}
# Load the dataset
file_path <- "data/PUBLIC_BIDMOVE_COMPLETE_20250118_0000000447644441.CSV"
raw_lines <- readLines(file_path)

# Step 2: Find where each table starts ("I" rows)
header_indices <- which(grepl("^I", raw_lines))  # Find all header rows
print(header_indices)  # Expected output: [1] 2 1823 (example)

# Step 3: Extract text for each table and read separately
if (length(header_indices) >= 1) {
  tables <- list()  # Initialize list to store tables
  
  for (i in seq_along(header_indices)) {
    start_idx <- header_indices[i]
    
    # Determine the end of the table (either next "I" or end of file)
    end_idx <- if (i < length(header_indices)) (header_indices[i + 1] - 1) else length(raw_lines)
    
    # Extract table text
    table_text <- raw_lines[start_idx:end_idx]
    
    # Read table into a data frame
    table_data <- fread(text = paste(table_text, collapse = "\n"), fill = TRUE)
    
    # Store table dynamically
    tables[[paste0("table", i, "PP")]] <- table_data
  }
} else {
  stop("Could not detect any tables in the file.")
}

```

```{r}
biddayoffer <- tables[["table1PP"]] |>
  filter(BIDTYPE == "ENERGY") |>
  select(BIDTYPE, BIDDAYOFFER_D, DUID, SETTLEMENTDATE, starts_with("PRICEBAND"))
bidperoffer <- tables[["table2PP"]] |>
  filter(BIDTYPE == "ENERGY") |>
  select(BIDTYPE, BIDPEROFFER_D, DUID, SETTLEMENTDATE, PERIODID, DIRECTION, starts_with("BANDAVAIL"), MAXAVAIL)
```

### Dispatch Summary ()

```{r}
file_path <- "data/PUBLIC_NEXT_DAY_DISPATCH_20250118_0000000447641515.csv"
raw_lines <- readLines(file_path)

# Step 2: Find where each table starts ("I" rows)
header_indices <- which(grepl("^I", raw_lines))  # Find all header rows
print(header_indices)  # Expected output: [1] 2 1823 (example)

# Step 3: Extract text for each table and read separately
if (length(header_indices) >= 1) {
  tables <- list()  # Initialize list to store tables
  
  for (i in seq_along(header_indices)) {
    start_idx <- header_indices[i]
    
    # Determine the end of the table (either next "I" or end of file)
    end_idx <- if (i < length(header_indices)) (header_indices[i + 1] - 1) else length(raw_lines)
    
    # Extract table text
    table_text <- raw_lines[start_idx:end_idx]
    
    # Read table into a data frame
    table_data <- fread(text = paste(table_text, collapse = "\n"), fill = TRUE)
    
    # Store table dynamically
    tables[[paste0("table", i, "PP")]] <- table_data
  }
} else {
  stop("Could not detect any tables in the file.")
}

```

```{r}
dispatchsum <- tables[["table1PP"]] |>
  select(UNIT_SOLUTION, DUID, DISPATCHINTERVAL, AVAILABILITY, TOTALCLEARED)
```

### Public Prices

```{r}
file_path <- "data/PUBLIC_PRICES_202501170000_20250118040501.csv"
raw_lines <- readLines(file_path)

tabletittle <- strsplit(raw_lines[1], ",")[[1]]

# Step 2: Find where each table starts ("I" rows)
header_indices <- which(grepl("^I", raw_lines))  # Find all header rows
print(header_indices)  # Expected output: [1] 2 1823 (example)

# Step 3: Extract text for each table and read separately
if (length(header_indices) >= 1) {
  tables <- list()  # Initialize list to store tables
  
  for (i in seq_along(header_indices)) {
    start_idx <- header_indices[i]
    
    # Determine the end of the table (either next "I" or end of file)
    end_idx <- if (i < length(header_indices)) (header_indices[i + 1] - 1) else length(raw_lines)
    
    # Extract table text
    table_text <- raw_lines[start_idx:end_idx]
    
    # Read table into a data frame
    table_data <- fread(text = paste(table_text, collapse = "\n"), fill = TRUE)
    
    # Store table dynamically
    tables[[paste0("table", i, tabletittle[9])]] <- table_data
  }
} else {
  stop("Could not detect any tables in the file.")
}

```

```{r}
publicprice <- tables[["table1PUBLIC_PRICES"]] |>
  select(DREGION, SETTLEMENTDATE, REGIONID, RRP, TOTALDEMAND, 
                             DEMANDFORECAST, DISPATCHABLEGENERATION, DISPATCHABLELOAD, NETINTERCHANGE, AVAILABLEGENERATION, INITIALSUPPLY) 
  #filter(REGIONID == "VIC1")
```

```{r}
totalDemandSum <- publicprice |>
  group_by(SETTLEMENTDATE) |>
  summarise(sumTotalDeman = sum(TOTALDEMAND, na.rm = TRUE)) |>
  ungroup()
```

```{r}
dispatchsumcleared <- dispatchsum |>
  group_by(DISPATCHINTERVAL) |>
  summarise(sumTotalCleared = sum(TOTALCLEARED, na.rm = TRUE)) |>
  ungroup()
```

```{r}
publicprice$SETTLEMENTDATE <- dmy_hm(publicprice$SETTLEMENTDATE)

biddayoffer$SETTLEMENTDATE <- trimws(biddayoffer$SETTLEMENTDATE)
biddayoffer$SETTLEMENTDATE <- ymd_hms(biddayoffer$SETTLEMENTDATE)

bidperoffer$SETTLEMENTDATE <- ymd_hms(bidperoffer$SETTLEMENTDATE)
bidperoffer <- bidperoffer %>%
  mutate(
    # Make sure SETTLEMENTDATE is Date class
    SETTLEMENTDATE = ymd(SETTLEMENTDATE),
    
    # Convert PERIODID (1–288) into 5-minute intervals
    INTERVAL_DATETIME = SETTLEMENTDATE + minutes((PERIODID - 1) * 5)
  )

# Convert integer64 to character to extract components
dispatchsum$DISPATCHINTERVAL <- as.character(dispatchsum$DISPATCHINTERVAL)
# Extract the date part (first 8 digits: YYYYMMDD)
dispatchsum$DATE <- substr(dispatchsum$DISPATCHINTERVAL, 1, 8)
# Extract the interval number (last 3 digits)
dispatchsum$INTERVAL <- as.numeric(substr(dispatchsum$DISPATCHINTERVAL, 9, 11))
# Convert DATE from YYYYMMDD to proper Date format
dispatchsum$DATE <- ymd(dispatchsum$DATE)
# Calculate the actual timestamp by adding intervals (5-minute steps)
dispatchsum$SETTLEMENTDATE <- dispatchsum$DATE + minutes((dispatchsum$INTERVAL - 1) * 5)
```

```{r}
dispatchsumgraph <- dispatchsum %>%
  mutate(HOUR = floor_date(SETTLEMENTDATE, unit = "hour")) %>%
  group_by(DUID, HOUR) %>%
  summarise(
    mean_cleared = mean(TOTALCLEARED, na.rm = TRUE),
    mean_availability = mean(AVAILABILITY, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(unused_capacity = mean_availability - mean_cleared)

```
