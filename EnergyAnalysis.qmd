```{r}
library(data.table)
library(forecast)
library(fabletools)
library(arrow)
library(stringr)
library(plotly)
library(feasts)
library(lubridate)
library(ggplot2)
library(stringr)
library(dplyr)
library(tidyr)
library(purrr)
library(tsibble)
library(factoextra)
library(umap)
```

#load dataset
```{r}
table_names <- c(
  "BIDDAYOFFER_D", "BIDPEROFFER_D", "DEMAND", "INTERMITTENT_DS_RUN",
  "INTERMITTENT_DS_PRED", "INTERMITTENT_FORECAST_TRK", "OPERATIONAL_DEMAND",
  "ROOFTOP", "UNIT_SOLUTION", "CONSTRAINT", "OUTAGEDETAIL", "PUBLIC_PRICES"
)

# Load all datasets into a named list
datasets <- lapply(table_names, function(name) {
  open_dataset(file.path("filtered_parquet_tables", name))
})

names(datasets) <- table_names

# Access like this:
datasets$UNIT_SOLUTION |>collect()
#datasets$PUBLIC_PRICES
#datasets$DEMAND |>collect() |>distinct(DUID)


```

### head of each clean table

```{r}
# Preview first 6 rows from each table
walk(names(datasets), function(tbl) {
  cat("\n\nüîç Table:", tbl, "\n")
  tryCatch({
    datasets[[tbl]] |>
      collect() |>
      head() |>
      print()
  }, error = function(e) {
    cat("‚ùå Error collecting data for", tbl, ":", e$message, "\n")
  })
})

```

##calculating residual demand
```{r}
# 1. Point at your on‚Äêdisk UNIT_SOLUTION (with pre‚Äêcomputed timestamp column)
us_ds <- open_dataset("filtered_parquet_tables/UNIT_SOLUTION")

# 2. In‚Äêplace Arrow aggregations: one row per REGIONID √ó timestamp
residual_demand_tbl <- us_ds %>%
  filter(!is.na(REGIONID)) %>%  
  group_by(REGIONID, timestamp) %>% 
  summarise(
    total_cleared = sum(TOTALCLEARED, na.rm = TRUE),
    wind_solar    = sum(
      if_else(FuelType %in% c("Wind - Wind", "Solar - Solar"),
              TOTALCLEARED,
              0),
      na.rm = TRUE
    ),
    .groups = "drop"
  ) |>
  # compute residual directly in Arrow
  mutate(residual_demand = total_cleared - wind_solar)

# 3. Pull back only that tiny summary into R
residual_demand_tbl |> collect() |> arrange(desc(timestamp))

```


# EDA

### Cov and cor analysis 
#### Able to choose region

```{r}
source("function_script/AggregationTime.R")
# ------------------ User Input: Region ------------------
# Use NULL to aggregate all regions
# Or use a vector of region IDs, e.g. region_filter <- c("VIC1", "NSW1")
region_filter <- "VIC1"

# ------------------ Aggregation ------------------
bidper_df_cov       <- safe_agg_by_region_and_time(datasets$BIDPEROFFER_D, grep("BANDAVAIL", names(datasets$BIDPEROFFER_D), value = TRUE), region_filter)
bidday_df_cov       <- safe_agg_by_region_and_time(datasets$BIDDAYOFFER_D, grep("PRICEBAND", names(datasets$BIDDAYOFFER_D), value = TRUE), region_filter)
demand_df_cov      <- safe_agg_by_region_and_time(datasets$DEMAND, "SCADA_VALUE", region_filter)
op_demand_df_cov    <- safe_agg_by_region_and_time(datasets$OPERATIONAL_DEMAND, "OPERATIONAL_DEMAND", region_filter)
rooftop_df_cov      <- safe_agg_by_region_and_time(datasets$ROOFTOP, c("POWERMEAN", "POWERPOE50"), region_filter)
prices_df_cov       <- safe_agg_by_region_and_time(datasets$PUBLIC_PRICES, c("RRP", "TOTALDEMAND", "DEMANDFORECAST", "DISPATCHABLEGENERATION", "DISPATCHABLELOAD", "NETINTERCHANGE", "AVAILABLEGENERATION", "INITIALSUPPLY"), region_filter)
constraint_df_cov   <- safe_agg_by_region_and_time(datasets$CONSTRAINT, c("VIOLATIONDEGREE", "MARGINALVALUE", "RHS", "LHS"), region_filter)
in_ds_pred_df_cov   <- safe_agg_by_region_and_time(datasets$INTERMITTENT_DS_PRED, c("FORECAST_MEAN", "FORECAST_POE50"), region_filter)
unit_df_cov         <- safe_agg_by_region_and_time(datasets$UNIT_SOLUTION, c("AVAILABILITY", "TOTALCLEARED"), region_filter)

# ------------------ Join All Tables ------------------
system_region_df <- reduce(
  list(bidper_df_cov, bidday_df_cov, demand_df_cov, op_demand_df_cov, rooftop_df_cov, prices_df_cov, constraint_df_cov, in_ds_pred_df_cov, unit_df_cov),
  full_join,
  by = c("REGIONID", "timestamp_30m")
)

# Add residual demand
system_region_df <- system_region_df |>
  mutate(residual_demand = OPERATIONAL_DEMAND - POWERMEAN)

system_scaled <- system_region_df |>
  select(-REGIONID, -timestamp_30m) |>
  scale()
```

```{r}
cov_matrix_region <- cov(system_scaled, use = "pairwise.complete.obs")
cor_matrix_region <- cor(system_scaled, use = "pairwise.complete.obs")

cov_matrix_region
cor_matrix_region

flatten_matrix <- function(mat) {
  mat[lower.tri(mat, diag = TRUE)] <- NA  # remove lower + diagonal
  df <- as.data.frame(as.table(mat))
  df <- na.omit(df)
  df <- df[order(-abs(df$Freq)), ]  # sort by absolute value descending
  names(df) <- c("Var1", "Var2", "Value")
  return(df)
}

# Top correlations
top_cor <- flatten_matrix(cor_matrix_region)
head(top_cor, 15)  # show top 10 strongest correlations

# Top covariances
top_cov <- flatten_matrix(cov_matrix_region)
head(top_cov, 15)  # show top 10 strongest covariances
```

```{r}
pca_result_system <- prcomp(system_scaled, center = TRUE, scale. = TRUE)
summary(pca_result_system)
```


#### Able to choose DUID
```{r}
# =======================
# üîÅ Load Aggregation Function
# =======================
source("function_script/AggregationTime.R")  # Make sure this file contains your enhanced `safe_agg_by_duid_and_time()`

# =======================
# ‚öôÔ∏è Variable Configuration
# =======================
aggregation_vars <- list(
  BIDDAYOFFER_D = grep("PRICEBAND", names(datasets$BIDDAYOFFER_D), value = TRUE),
  BIDPEROFFER_D = c(grep("BANDAVAIL", names(datasets$BIDPEROFFER_D), value = TRUE), "MAXAVAIL"),
  UNIT_SOLUTION = c("AVAILABILITY", "TOTALCLEARED"),
  DEMAND = "SCADA_VALUE",
  INTERMITTENT_DS_PRED = c("FORECAST_MEAN", "FORECAST_POE50"),
  CONSTRAINT = c("VIOLATIONDEGREE", "RHS", "LHS", "MARGINALVALUE"),
  PUBLIC_PRICES = c("RRP", "TOTALDEMAND", "DEMANDFORECAST", "DISPATCHABLEGENERATION",
                    "DISPATCHABLELOAD", "NETINTERCHANGE", "AVAILABLEGENERATION", "INITIALSUPPLY"),
  OPERATIONAL_DEMAND = "OPERATIONAL_DEMAND",
  ROOFTOP = c("POWERMEAN", "POWERPOE50")
)

# =======================
# üß† User Input
# =======================
duid_filter <- "NPS"  # or NULL to aggregate across all DUIDs

# =======================
# üîÑ Aggregation Loop
# =======================
aggregated_list <- lapply(names(aggregation_vars), function(tbl_name) {
  if (!tbl_name %in% names(datasets)) return(NULL)
  
  vars <- aggregation_vars[[tbl_name]]
  df <- datasets[[tbl_name]]
  
  message("üìä Aggregating ", tbl_name)
  safe_agg_by_duid_and_time(df, vars, duid_filter)
})

# Remove any null results
aggregated_list <- aggregated_list[!sapply(aggregated_list, is.null)]

# =======================
# üîó Dynamic Join of Tables
# =======================

# Start with the first table
unit_level_df <- aggregated_list[[1]]

# Loop through the rest and join based on common columns
for (i in 2:length(aggregated_list)) {
  next_df <- aggregated_list[[i]]
  
  # Get common join keys (must include timestamp_30m)
  join_keys <- intersect(names(unit_level_df), names(next_df))
  join_keys <- join_keys[join_keys %in% c("REGIONID", "DUID", "timestamp_30m")]
  
  if (length(join_keys) < 1) {
    warning(paste("‚ö†Ô∏è Skipping join with dataset", i, "- no common join keys"))
    next
  }
  
  unit_level_df <- full_join(unit_level_df, next_df, by = join_keys)
}

# =======================
# ‚öôÔ∏è Scale Data & Compute Covariance/Correlation
# =======================
unit_scaled <- unit_level_df |>
  select(-REGIONID, -DUID, -timestamp_30m) |>
  scale()
```

```{r}
cov_matrix_unit <- cov(unit_scaled, use = "pairwise.complete.obs")
cor_matrix_unit <- cor(unit_scaled, use = "pairwise.complete.obs")

# =======================
# üì§ Output
# =======================
print("üìà Covariance Matrix:")
print(cov_matrix_unit)

print("üîó Correlation Matrix:")
print(cor_matrix_unit)

# (Optional) Save to CSV
# write.csv(cov_matrix_unit, "cov_matrix.csv")
# write.csv(cor_matrix_unit, "cor_matrix.csv")
# Flatten the upper triangle of a matrix into a data frame
flatten_matrix <- function(mat) {
  mat[lower.tri(mat, diag = TRUE)] <- NA  # remove lower + diagonal
  df <- as.data.frame(as.table(mat))
  df <- na.omit(df)
  df <- df[order(-abs(df$Freq)), ]  # sort by absolute value descending
  names(df) <- c("Var1", "Var2", "Value")
  return(df)
}

# Top correlations
top_cor <- flatten_matrix(cor_matrix_unit)
head(top_cor, 15)  # show top 10 strongest correlations

# Top covariances
top_cov <- flatten_matrix(cov_matrix_unit)
head(top_cov, 15)  # show top 10 strongest covariances

```


```{r}
#PLS
library(pls)
pls_model <- plsr(RRP ~ ., data = as.data.frame(unit_scaled_logged), ncomp = 5)
summary(pls_model)

```
### PCA
#### normal
```{r}
prepare_unit_scaled <- function(unit_level_df) {
  # Step 1: Drop ID columns
  df <- unit_level_df |>
    select(-timestamp_30m, -REGIONID, -DUID)
  
  # Step 2: Remove columns with all NA
  df <- df[, colSums(is.na(df)) < nrow(df)]
  
  # Step 3: Remove constant columns
  df <- df[, apply(df, 2, function(x) sd(x, na.rm = TRUE) > 0)]
  
  # Step 4: Impute NA with column means
  df <- as.data.frame(apply(df, 2, function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x)))
  
  # Step 5: Scale
  scale(df)
}
# Run PCA
unit_scaled_clean <- prepare_unit_scaled(unit_level_df)
pca_result <- prcomp(unit_scaled_clean, center = TRUE, scale. = TRUE)


# View variance explained
summary(pca_result)


```
#### Log
```{r}
log_transform_safe <- function(x) {
  if (any(x < 0, na.rm = TRUE)) {
    return(x)  # Skip if negative values exist
  } else {
    return(log1p(x))  # log1p(x) = log(1 + x), avoids log(0)
  }
}

prepare_unit_scaled_logged <- function(df) {
  df <- df |>
    select(-timestamp_30m, -REGIONID, -DUID)
  
  # Remove all-NA columns
  df <- df[, colSums(is.na(df)) < nrow(df)]
  
  # Apply log1p transformation to each numeric column
  df <- as.data.frame(lapply(df, log_transform_safe))
  
  # Remove constant columns
  df <- df[, apply(df, 2, function(x) sd(x, na.rm = TRUE) > 0)]
  
  # Impute NA with column means
  df <- as.data.frame(lapply(df, function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x)))
  
  # Final scale
  scale(df)
}

# Usage
unit_scaled_logged <- prepare_unit_scaled_logged(unit_level_df)
pca_logged <- prcomp(unit_scaled_logged)
summary(pca_logged)
```






## shiny app - dashboard

animated bid stack changes enable dynamic filtering handling very large dataset, donwloading and data cleaning - timeseries analysis

## combined analysis with dispatchsum and publicprice, also other dataset

## analyse bidding strategies

## give region for each duid

avalibility vs total cleared (difference that is not dispatched ) - shifting probably with price (high closer to one)-need to normalise. human error etc.

correlates with the price, problably will highlight the external factor, the corr maybe not will be showing 0.5 or 0.7

find the correlation the number

find other correlation that is 0.4 or 0.1 find the correlation just any so

residual demand (demand that is not meet by renewable, wind and solar ) higher residual demand (change to graph 3)

is residual demand effecting the price shifting.
